% ============================================================================
% APPENDIX: ULTRA-DETAILED ALGORITHM WORKFLOW (V2 - REORDERED)
% NichePopulation: A Complete Visual and Numerical Guide
% ============================================================================
% This version reorders content for better pedagogical flow:
% - Definitions BEFORE usage
% - Motivation BEFORE mechanics
% - "Why" BEFORE "How"
% ============================================================================

\subsection{Algorithm Workflow: Complete Visual Guide}
\label{sec:workflow}

This appendix provides an exhaustive, self-contained walkthrough of the NichePopulation algorithm. After reading this section, you will fully understand every step of the algorithm and how specialization emerges from competition.

% ============================================================================
% SECTION 1: WHAT IS SPECIALIZATION? (DEFINITIONS FIRST)
% ============================================================================

\subsubsection{What Is Specialization?}

Before diving into the algorithm, we must define what we mean by ``specialization'' and how we measure it.

\paragraph{The Core Idea.} In a multi-agent system operating across different environmental conditions (called \textbf{regimes}), an agent is \textit{specialized} if it focuses on a subset of regimes rather than being equally capable across all of them.

\paragraph{Niche Affinity ($\alpha$).} Each agent maintains a probability distribution $\alpha = (\alpha_1, \ldots, \alpha_R)$ over $R$ regimes, representing its ``preference'' or ``expertise'' for each regime. This vector always sums to 1.

\begin{itemize}
    \item \textbf{Generalist}: $\alpha = (0.25, 0.25, 0.25, 0.25)$ --- equal expertise everywhere
    \item \textbf{Specialist}: $\alpha = (0.82, 0.06, 0.06, 0.06)$ --- concentrated expertise in one regime
\end{itemize}

\paragraph{Specialization Index (SI).} We quantify specialization using an entropy-based metric:

\begin{equation}
\boxed{\text{SI}(\alpha) = 1 - \frac{H(\alpha)}{\log R}}
\end{equation}

where $H(\alpha) = -\sum_r \alpha_r \log \alpha_r$ is the Shannon entropy.

\textbf{Interpretation:}
\begin{itemize}
    \item $\text{SI} = 0$: Perfect generalist (uniform distribution, maximum entropy)
    \item $\text{SI} = 1$: Perfect specialist (all probability on one regime, zero entropy)
    \item $\text{SI} \in (0, 1)$: Partial specialization
\end{itemize}

\paragraph{SI Calculation Example.}

For an agent with $\alpha = (0.82, 0.06, 0.06, 0.06)$ over $R = 4$ regimes:

\textbf{Step 1}: Calculate entropy
\begin{align}
H(\alpha) &= -0.82 \log(0.82) - 3 \times 0.06 \log(0.06) \\
&= 0.162 + 0.506 = 0.668 \text{ nats}
\end{align}

\textbf{Step 2}: Normalize by maximum entropy ($H_{\max} = \log 4 = 1.386$)

\textbf{Step 3}: Compute SI
\begin{equation}
\text{SI} = 1 - \frac{0.668}{1.386} = 1 - 0.482 = \boxed{0.518}
\end{equation}

\textit{This agent is 51.8\% specialized toward one regime.}

% ============================================================================
% SECTION 2: WHY SPECIALIZATION? (MOTIVATION)
% ============================================================================

\subsubsection{Why Do We Want Specialization?}

\paragraph{The Problem with Homogeneity.} If all agents adopt the same strategy, they:
\begin{enumerate}
    \item Compete directly with each other (crowding)
    \item All fail together when that strategy's regime disappears
    \item Miss opportunities in other regimes
\end{enumerate}

\paragraph{The Value of Diversity.} A population with diverse specialists:
\begin{enumerate}
    \item Has at least one expert for every regime
    \item Is robust to regime changes (always someone ready)
    \item Achieves better collective performance (+26.5\% in our experiments)
\end{enumerate}

\paragraph{The Research Question.} Can agents \textit{spontaneously} develop diverse specializations without:
\begin{itemize}
    \item Explicit communication?
    \item Central coordination?
    \item Handcrafted diversity rewards?
\end{itemize}

\textbf{Our answer: Yes.} Competition alone is sufficient.

% ============================================================================
% SECTION 3: THE KEY MECHANISM (INTUITION BEFORE MATH)
% ============================================================================

\subsubsection{The Key Mechanism: Competition Creates Diversity}

The NichePopulation algorithm induces specialization through three interlocking mechanisms:

\paragraph{Mechanism 1: Winner-Take-All Competition.}

In each iteration, only the \textbf{single best-performing agent} receives updates. All others are ``frozen''---they learn nothing from that round.

\textbf{Why this matters:}
\begin{itemize}
    \item If two agents have identical strategies, they compete directly
    \item Only one can win; the other learns nothing
    \item The loser cannot ``copy'' the winner's improvement
    \item This prevents convergence to a single homogeneous strategy
\end{itemize}

\paragraph{Mechanism 2: Niche Affinity Tracking.}

When an agent wins in a particular regime, its affinity for that regime \textit{increases}. Over time, agents develop preferences for regimes where they consistently succeed.

\paragraph{Mechanism 3: Niche Bonus (Optional Accelerator).}

Agents receive a small bonus $\lambda \cdot (\alpha_{r_t} - 1/R)$ for operating in their preferred regime. This accelerates specialization but is \textit{not required}---specialization emerges even when $\lambda = 0$.

\paragraph{The Result: Competitive Exclusion.}

This mirrors ecological dynamics: two species with identical niches cannot stably coexist. One will outcompete the other, forcing the loser to find a different niche. Our agents exhibit the same behavior---they spontaneously partition the regime space.

% ============================================================================
% SECTION 4: THE GROUND TRUTH (WHAT AGENTS MUST LEARN)
% ============================================================================

\subsubsection{The Ground Truth: Regime-Method Affinity Matrix}

The environment contains a hidden ``ground truth'' that agents must discover: different \textbf{methods} (prediction strategies) work better in different \textbf{regimes} (environmental conditions).

\begin{table}[h]
\centering
\small
\caption{Crypto domain affinity matrix $A(r, m) \in [0,1]$. Higher values = better performance. Bold = best method for that regime.}
\begin{tabular}{l|cccc|l}
\toprule
\textbf{Method} & \textbf{Bull} & \textbf{Bear} & \textbf{Sideways} & \textbf{Volatile} & \textbf{Best For} \\
\midrule
Naive & 0.50 & 0.30 & 0.60 & 0.40 & --- \\
Momentum-Short & 0.80 & 0.70 & 0.40 & 0.60 & --- \\
Momentum-Long & \textbf{0.90} & \textbf{0.80} & 0.30 & 0.50 & Bull, Bear \\
Mean-Revert & 0.30 & 0.40 & \textbf{0.90} & \textbf{0.70} & Sideways, Volatile \\
Trend & 0.85 & 0.75 & 0.35 & 0.50 & --- \\
\bottomrule
\end{tabular}
\label{tab:affinity}
\end{table}

\textbf{Key insight}: No single method dominates all regimes. Momentum-Long excels in Bull/Bear but fails in Sideways. Mean-Revert is the opposite. This heterogeneity creates the \textit{opportunity} for specialization.

% ============================================================================
% SECTION 5: INITIALIZATION (THE BLANK SLATE)
% ============================================================================

\subsubsection{Phase 0: Initialization (The Blank Slate)}

All $N = 8$ agents begin \textbf{identically}---perfect generalists with no knowledge.

\paragraph{Data Structure 1: Method Beliefs ($\beta$).}

Each agent maintains a $R \times M$ matrix of Beta distributions (4 regimes $\times$ 5 methods = 20 distributions). Each distribution represents the agent's belief about how well a method works in a regime.

\textbf{Initial value}: $\text{Beta}(1, 1)$ for all method-regime pairs.

\textit{Why Beta(1,1)?} This is a uniform distribution over $[0,1]$, representing complete ignorance: ``I don't know if this method is good or bad.''

\paragraph{Data Structure 2: Niche Affinity ($\alpha$).}

Each agent maintains a probability vector over regimes.

\textbf{Initial value}: $\alpha = (1/R, \ldots, 1/R) = (0.25, 0.25, 0.25, 0.25)$

\textit{Why uniform?} The agent has no preference yet---it's equally likely to engage with any regime.

\begin{table}[h]
\centering
\small
\caption{Initial state of all 8 agents at $t=0$. All agents are identical generalists with SI = 0.}
\begin{tabular}{l|cccc|c|l}
\toprule
\textbf{Agent} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} & \textbf{Status} \\
\midrule
Agent 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 1 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 2 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 3 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 4 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 5 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 6 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 7 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
\bottomrule
\end{tabular}
\label{tab:init}
\end{table}

% ============================================================================
% SECTION 6: MASTER FLOWCHART
% ============================================================================

\subsubsection{The Algorithm: Master Flowchart}

Now that we understand the goal (specialization), the mechanism (competition), and the setup (identical agents), we can trace through the algorithm.

\begin{figure}[h!]
    \centering
    \resizebox{0.95\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=1.0cm,
        init/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=blue!60, fill=blue!8, rounded corners, font=\small},
        process/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=black, fill=white, rounded corners, font=\small},
        compute/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=purple!60, fill=purple!8, rounded corners, font=\small},
        decision/.style={diamond, aspect=2.2, minimum width=2.5cm, text centered, draw=orange!70, fill=orange!10, font=\small},
        winner/.style={rectangle, minimum width=3.0cm, minimum height=0.9cm, text centered, draw=green!60!black, fill=green!10, rounded corners, font=\small},
        loser/.style={rectangle, minimum width=2.5cm, minimum height=0.8cm, text centered, draw=red!60, fill=red!10, rounded corners, font=\small},
        arrow/.style={-{Stealth[length=2.5mm]}, thick},
    ]

    % === PHASE 0: INITIALIZATION ===
    \node (beta) [init] {Init $\beta_{i,r,m}^+ = \beta_{i,r,m}^- = 1$\\(20 Beta distributions)};
    \node (alpha) [init, right=1.5cm of beta] {Init $\alpha_{i,r} = 1/R$\\$[0.25, 0.25, 0.25, 0.25]$};
    
    % === PHASE 1: CONTEXT ===
    \node (regime) [process, below=1.8cm of $(beta)!0.5!(alpha)$] {Sample regime $r_t \sim \pi(r)$\\Example: $r_t = \text{Bull}$};
    \node (filter) [process, below=0.8cm of regime] {Filter beliefs for $r_t$\\(5 Beta dists for Bull)};
    
    % === PHASE 2: SELECTION ===
    \node (thompson) [compute, below=1.2cm of filter] {Thompson Sampling:\\$\tilde{\theta}_m \sim \text{Beta}(\beta^+_{r_t,m}, \beta^-_{r_t,m})$};
    \node (select) [compute, below=0.8cm of thompson] {Select method:\\$m_i = \arg\max_m \tilde{\theta}_m$};
    
    % === PHASE 3: EXECUTION ===
    \node (execute) [process, below=1.2cm of select] {Execute method $m_i$\\in environment};
    \node (reward) [process, below=0.8cm of execute] {Raw reward:\\$R_i = A(r_t, m_i) + \epsilon$};
    
    % === PHASE 4: COMPETITION ===
    \node (bonus) [compute, below=1.2cm of reward] {Niche bonus:\\$B_i = \lambda(\alpha_{i,r_t} - 1/R)$};
    \node (score) [compute, below=0.8cm of bonus] {Final score:\\$S_i = R_i + B_i$};
    \node (compare) [decision, below=1.0cm of score] {$S_i > S_j$ $\forall j$?};
    
    % === PHASE 5: UPDATES ===
    \node (losernode) [loser, right=2.5cm of compare] {LOSER\\(No update)};
    \node (skillup) [winner, below left=1.5cm and 0cm of compare] {Skill update:\\$\beta^+_{r_t,m_i} \mathrel{+}= 1$};
    \node (affup) [winner, below right=1.5cm and 0cm of compare] {Affinity update:\\$\alpha_{r_t} \mathrel{+}= \eta(1-\alpha_{r_t})$};
    \node (normalize) [winner, below=0.8cm of $(skillup)!0.5!(affup)$] {Normalize: $\alpha \leftarrow \alpha / \|\alpha\|_1$};
    
    % === LOOP BACK ===
    \node (nextiter) [process, below=0.8cm of normalize] {Next iteration $t \leftarrow t+1$};
    
    % === ARROWS ===
    \draw [arrow] (beta) -- (regime);
    \draw [arrow] (alpha) -- (regime);
    \draw [arrow] (regime) -- (filter);
    \draw [arrow] (filter) -- (thompson);
    \draw [arrow] (thompson) -- (select);
    \draw [arrow] (select) -- (execute);
    \draw [arrow] (execute) -- (reward);
    \draw [arrow] (reward) -- (bonus);
    \draw [arrow] (bonus) -- (score);
    \draw [arrow] (score) -- (compare);
    \draw [arrow] (compare) -- node[above, font=\footnotesize] {No} (losernode);
    \draw [arrow] (compare) -| node[near start, left, font=\footnotesize] {Yes} (skillup);
    \draw [arrow] (compare) -| (affup);
    \draw [arrow] (skillup) -- (normalize);
    \draw [arrow] (affup) -- (normalize);
    \draw [arrow] (normalize) -- (nextiter);
    \draw [arrow, rounded corners] (nextiter.west) -- +(-3.5,0) |- (regime.west);
    
    % Phase labels
    \node [above=0.3cm of beta, font=\footnotesize\bfseries, blue!70] {PHASE 0: INIT};
    \node [left=0.1cm of regime, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 1};
    \node [left=0.1cm of thompson, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 2};
    \node [left=0.1cm of execute, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 3};
    \node [left=0.1cm of bonus, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 4};
    \node [left=0.1cm of skillup, font=\footnotesize\bfseries, rotate=90, anchor=south, green!50!black] {PHASE 5};

    \end{tikzpicture}
    }
    \caption{Complete NichePopulation flowchart. Blue = initialization, Purple = computation, Green = winner-only updates, Red = loser path. Loop repeats for $T=500$ iterations.}
    \label{fig:workflow}
\end{figure}

% ============================================================================
% SECTION 7: COMPLETE SINGLE ITERATION TRACE
% ============================================================================

\subsubsection{One Complete Iteration: Step-by-Step Trace}

We now trace through \textbf{Iteration 1} showing all 8 agents' computations.

\paragraph{Step 1: Environment samples regime.}
\begin{equation}
r_t = \text{Bull} \quad \text{(sampled with probability } \pi(\text{Bull}) = 0.30\text{)}
\end{equation}

\paragraph{Step 2: Each agent selects a method via Thompson Sampling.}

Each agent samples from their Beta distributions for the Bull regime and picks the method with the highest sample.

\begin{table}[h]
\centering
\small
\caption{Iteration 1: Thompson Sampling method selection. With uniform priors Beta(1,1), samples are random.}
\begin{tabular}{l|ccccc|c}
\toprule
\textbf{Agent} & $\tilde{\theta}_{\text{naive}}$ & $\tilde{\theta}_{\text{mom-s}}$ & $\tilde{\theta}_{\text{mom-l}}$ & $\tilde{\theta}_{\text{mr}}$ & $\tilde{\theta}_{\text{trend}}$ & \textbf{Selected} \\
\midrule
Agent 0 & 0.42 & 0.68 & 0.55 & 0.31 & \textbf{0.73} & Trend \\
Agent 1 & 0.51 & 0.44 & \textbf{0.82} & 0.29 & 0.61 & Mom-Long \\
Agent 2 & 0.38 & \textbf{0.77} & 0.69 & 0.45 & 0.52 & Mom-Short \\
Agent 3 & 0.63 & 0.41 & 0.58 & \textbf{0.71} & 0.39 & Mean-Rev \\
Agent 4 & \textbf{0.85} & 0.33 & 0.47 & 0.52 & 0.44 & Naive \\
Agent 5 & 0.29 & 0.56 & 0.64 & 0.48 & \textbf{0.78} & Trend \\
Agent 6 & 0.44 & 0.62 & \textbf{0.71} & 0.35 & 0.59 & Mom-Long \\
Agent 7 & 0.57 & \textbf{0.79} & 0.66 & 0.42 & 0.51 & Mom-Short \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Step 3: Execute methods and receive rewards.}

Each agent's raw reward is: $R_i = A(r_t, m_i) + \epsilon_i$, where $\epsilon_i \sim \mathcal{N}(0, 0.15^2)$

\paragraph{Step 4: Calculate niche bonus.}

The niche bonus formula is: $B_i = \lambda \cdot (\alpha_{i,r_t} - 1/R)$

Since all agents start with $\alpha_{r_t} = 0.25 = 1/R$, all bonuses are \textbf{zero} in iteration 1.

\begin{table}[h]
\centering
\small
\caption{Iteration 1: Score calculation. With $\lambda = 0.3$ and uniform affinities, all bonuses = 0.}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\textbf{Agent} & \textbf{Method} & $A(r_t, m)$ & $\epsilon$ & $R_i$ & $B_i$ & $S_i = R_i + B_i$ \\
\midrule
Agent 0 & Trend & 0.85 & +0.03 & 0.88 & 0.00 & 0.88 \\
\rowcolor{green!15} Agent 1 & Mom-Long & 0.90 & +0.07 & \textbf{0.97} & 0.00 & \textbf{0.97} $\leftarrow$ Winner \\
Agent 2 & Mom-Short & 0.80 & -0.05 & 0.75 & 0.00 & 0.75 \\
Agent 3 & Mean-Rev & 0.30 & +0.02 & 0.32 & 0.00 & 0.32 \\
Agent 4 & Naive & 0.50 & +0.11 & 0.61 & 0.00 & 0.61 \\
Agent 5 & Trend & 0.85 & -0.08 & 0.77 & 0.00 & 0.77 \\
Agent 6 & Mom-Long & 0.90 & -0.02 & 0.88 & 0.00 & 0.88 \\
Agent 7 & Mom-Short & 0.80 & +0.04 & 0.84 & 0.00 & 0.84 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Step 5: Winner-Take-All.}

Agent 1 wins with $S_1 = 0.97$. \textbf{Only Agent 1 updates. All others remain frozen.}

\paragraph{Step 6: Winner's updates.}

\textbf{(a) Method belief update} (Agent 1 reinforces Momentum-Long in Bull):
\begin{equation}
\beta^+_{1,\text{Bull},\text{Mom-Long}} = 1 + 1 = 2, \quad \beta^-_{1,\text{Bull},\text{Mom-Long}} = 1
\end{equation}
The belief shifts from Beta(1,1) $\to$ Beta(2,1), with mean increasing from 0.50 to 0.67.

\textbf{(b) Niche affinity update} (Agent 1 becomes more ``Bull-oriented''):
\begin{align}
\alpha_{1,\text{Bull}}^{\text{new}} &= 0.25 + 0.1 \times (1 - 0.25) = 0.325 \\
\alpha_{1,r}^{\text{new}} &= \max(0.01, 0.25 - 0.1/3) = 0.217 \quad \text{for } r \neq \text{Bull}
\end{align}

\textbf{(c) Normalize} to ensure $\sum \alpha = 1$:
\begin{equation}
\alpha_1 = (0.333, 0.222, 0.222, 0.222) \quad \text{SI} = 0.041
\end{equation}

\begin{table}[h]
\centering
\small
\caption{Agent states after Iteration 1. Only Agent 1 has changed (highlighted).}
\begin{tabular}{l|cccc|c|l}
\toprule
\textbf{Agent} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} & \textbf{Change} \\
\midrule
Agent 0 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
\rowcolor{green!10} Agent 1 & \textbf{0.333} & 0.222 & 0.222 & 0.222 & 0.041 & $\uparrow$ Bull \\
Agent 2 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 3 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 4 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 5 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 6 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 7 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% SECTION 8: MULTI-ITERATION CONVERGENCE
% ============================================================================

\subsubsection{Multi-Iteration Convergence: How Agents Diverge}

The key phenomenon: \textbf{random early wins create path dependence}.

\begin{itemize}
    \item Agent 1 wins a few Bull rounds $\to$ develops Bull expertise
    \item Agent 2 wins a few Bear rounds $\to$ develops Bear expertise
    \item Agent 0 wins a few Sideways rounds $\to$ develops Sideways expertise
    \item \ldots and so on
\end{itemize}

Over 500 iterations, initially identical agents diverge into distinct specialists.

\begin{table}[h]
\centering
\small
\caption{Agent specialization trajectories. Bold values show the emerging primary niche.}
\begin{tabular}{l|c|cccc|c}
\toprule
& \textbf{Iter} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} \\
\midrule
\multirow{4}{*}{\textbf{Agent 0}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.18 & 0.22 & \textbf{0.38} & 0.22 & 0.08 \\
& 200 & 0.12 & 0.15 & \textbf{0.58} & 0.15 & 0.31 \\
& 500 & 0.08 & 0.09 & \textbf{0.75} & 0.08 & 0.58 \\
\midrule
\multirow{4}{*}{\textbf{Agent 1}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & \textbf{0.42} & 0.19 & 0.20 & 0.19 & 0.12 \\
& 200 & \textbf{0.65} & 0.12 & 0.12 & 0.11 & 0.42 \\
& 500 & \textbf{0.82} & 0.06 & 0.06 & 0.06 & 0.71 \\
\midrule
\multirow{4}{*}{\textbf{Agent 2}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.20 & \textbf{0.35} & 0.23 & 0.22 & 0.06 \\
& 200 & 0.11 & \textbf{0.55} & 0.18 & 0.16 & 0.28 \\
& 500 & 0.05 & \textbf{0.78} & 0.10 & 0.07 & 0.62 \\
\midrule
\multirow{4}{*}{\textbf{Agent 3}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.21 & 0.20 & 0.22 & \textbf{0.37} & 0.07 \\
& 200 & 0.14 & 0.13 & 0.15 & \textbf{0.58} & 0.30 \\
& 500 & 0.07 & 0.07 & 0.09 & \textbf{0.77} & 0.60 \\
\bottomrule
\end{tabular}
\label{tab:convergence}
\end{table}

\textbf{Result}: The population has \textit{partitioned} the regime space:
\begin{itemize}
    \item Agent 1: Bull specialist (SI = 0.71)
    \item Agent 2: Bear specialist (SI = 0.62)
    \item Agent 0: Sideways specialist (SI = 0.58)
    \item Agent 3: Volatile specialist (SI = 0.60)
\end{itemize}

% ============================================================================
% SECTION 9: WHY LAMBDA=0 WORKS
% ============================================================================

\subsubsection{The Core Insight: Why $\lambda = 0$ Still Works}

\textbf{The question}: If there's no niche bonus ($\lambda = 0$), why do agents still specialize?

\textbf{The answer}: Competition alone creates path dependence through four mechanisms:

\begin{enumerate}
    \item \textbf{Random initial wins}: In early iterations, winners are determined by noise $\epsilon$. By chance, different agents win in different regimes.
    
    \item \textbf{Winner-take-all prevents convergence}: Losers don't update, so they cannot copy the winner's strategy.
    
    \item \textbf{Method learning creates skill gaps}: Winners accumulate better beliefs about what works in ``their'' regime. Their Beta distributions narrow around effective methods.
    
    \item \textbf{Skill gaps compound}: Once Agent A is slightly better at Bull, they win more Bull rounds, further improving their Bull skills. This is a positive feedback loop.
\end{enumerate}

\begin{table}[h]
\centering
\small
\caption{SI comparison: $\lambda = 0$ vs $\lambda = 0.3$. Competition alone ($\lambda = 0$) produces SI $> 0.25$, significantly above the random baseline of 0.13.}
\begin{tabular}{l|cc|c}
\toprule
\textbf{Domain} & $\lambda = 0$ SI & $\lambda = 0.3$ SI & $\lambda$ Boost \\
\midrule
Crypto & 0.31 & 0.75 & +142\% \\
Commodities & 0.30 & 0.78 & +160\% \\
Weather & 0.31 & 0.71 & +129\% \\
Solar & 0.26 & 0.82 & +215\% \\
Traffic & 0.29 & 0.68 & +134\% \\
Air Quality & 0.50 & 0.80 & +60\% \\
\midrule
\textbf{Average} & \textbf{0.33} & \textbf{0.76} & +140\% \\
\textbf{vs Random (0.13)} & \textbf{+154\%} & \textbf{+485\%} & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Conclusion}: The niche bonus $\lambda$ \textit{accelerates} specialization but does not \textit{cause} it. \textbf{Competition alone is sufficient.}

% ============================================================================
% SECTION 10: COMPETITION DYNAMICS
% ============================================================================

\subsubsection{Competition Dynamics: Crowding and Migration}

\textbf{What happens if two agents specialize in the same regime?}

Suppose Agents 1 and 6 both become Bull specialists. When a Bull round occurs:
\begin{equation}
\E[\text{Payoff for each Bull specialist}] = \frac{V_{\text{Bull}}}{2} \quad \text{(split the reward)}
\end{equation}

Meanwhile, Agent 3 (sole Volatile specialist) gets:
\begin{equation}
\E[\text{Payoff in Volatile}] = V_{\text{Volatile}} \quad \text{(no competition)}
\end{equation}

\textbf{Result}: It's more profitable to be the sole specialist in an uncrowded niche. Over time, the weaker Bull specialist will ``migrate'' to Volatile after repeatedly losing to the stronger Bull specialist.

This is \textbf{competitive exclusion} from ecology: identical strategies cannot stably coexist.

% ============================================================================
% SECTION 11: SUMMARY
% ============================================================================

\subsubsection{Summary: The Complete Picture}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={rectangle, draw, rounded corners, minimum width=3.5cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=2.5mm]}, thick},
]

% Left column: Inputs
\node (env) [box, fill=blue!10] {Environment\\(4 regimes)};
\node (methods) [box, fill=blue!10, below=of env] {Methods\\(5 strategies)};
\node (agents) [box, fill=blue!10, below=of methods] {Agents\\(8 identical)};

% Middle: Mechanism
\node (compete) [box, fill=orange!15, right=2.5cm of methods] {Competition\\(winner-take-all)};
\node (update) [box, fill=orange!15, below=of compete] {Selective Update\\(only winner learns)};
\node (affinity) [box, fill=orange!15, above=of compete] {Niche Affinity\\(regime preference)};

% Right column: Outputs
\node (special) [box, fill=green!15, right=2.5cm of compete] {Specialization\\(SI $\approx$ 0.75)};
\node (diverse) [box, fill=green!15, above=of special] {Diversity\\(different niches)};
\node (robust) [box, fill=green!15, below=of special] {Robustness\\(+26.5\% perf)};

% Arrows
\draw [arrow] (env) -- (affinity);
\draw [arrow] (methods) -- (compete);
\draw [arrow] (agents) -- (update);
\draw [arrow] (affinity) -- (compete);
\draw [arrow] (compete) -- (update);
\draw [arrow] (update.east) -- ++(0.5,0) |- (affinity.east);
\draw [arrow] (compete) -- (special);
\draw [arrow] (special) -- (diverse);
\draw [arrow] (special) -- (robust);

% Labels
\node [above=0.2cm of env, font=\footnotesize\bfseries] {INPUTS};
\node [above=0.2cm of affinity, font=\footnotesize\bfseries] {MECHANISM};
\node [above=0.2cm of diverse, font=\footnotesize\bfseries] {OUTPUTS};

\end{tikzpicture}
\caption{High-level summary: identical agents + competition $\rightarrow$ emergent specialization.}
\label{fig:summary}
\end{figure}

\paragraph{Key Takeaways.}
\begin{enumerate}
    \item \textbf{Competition is the source}: Specialization emerges from winner-take-all dynamics, not explicit diversity incentives.
    \item \textbf{No communication needed}: Agents differentiate through individual learning, not coordination.
    \item \textbf{Ecologically inspired}: The mechanism mirrors competitive exclusion in natural ecosystems.
    \item \textbf{$\lambda = 0$ still works}: The niche bonus accelerates but does not cause specialization.
    \item \textbf{Robust and general}: Works across 6 diverse real-world domains with effect sizes $d > 15$.
\end{enumerate}

