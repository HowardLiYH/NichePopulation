% ============================================================================
% APPENDIX: ULTRA-DETAILED ALGORITHM WORKFLOW
% NichePopulation: A Complete Visual and Numerical Guide
% ============================================================================
% This appendix provides a self-contained, comprehensive explanation of the
% NichePopulation algorithm through detailed diagrams and complete numerical
% examples. After reading this appendix, you will fully understand every step
% of the algorithm without needing to reference the main paper.
% ============================================================================

\subsection{Algorithm Workflow: Complete Visual Guide}
\label{sec:workflow}

This section provides an exhaustive walkthrough of the NichePopulation algorithm. We trace through complete iterations with all 8 agents, showing exactly how specialization emerges from competition.

% ============================================================================
% SECTION 1: MASTER FLOWCHART
% ============================================================================

\subsubsection{Master Algorithm Flowchart}

\begin{figure}[h!]
    \centering
    \resizebox{0.95\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=1.0cm,
        % Node styles
        init/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=blue!60, fill=blue!8, rounded corners, font=\small},
        process/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=black, fill=white, rounded corners, font=\small},
        compute/.style={rectangle, minimum width=3.2cm, minimum height=0.9cm, text centered, draw=purple!60, fill=purple!8, rounded corners, font=\small},
        decision/.style={diamond, aspect=2.2, minimum width=2.5cm, text centered, draw=orange!70, fill=orange!10, font=\small},
        winner/.style={rectangle, minimum width=3.0cm, minimum height=0.9cm, text centered, draw=green!60!black, fill=green!10, rounded corners, font=\small},
        loser/.style={rectangle, minimum width=2.5cm, minimum height=0.8cm, text centered, draw=red!60, fill=red!10, rounded corners, font=\small},
        arrow/.style={-{Stealth[length=2.5mm]}, thick},
        dataarrow/.style={-{Stealth[length=2mm]}, thick, dashed, blue!60},
    ]

    % === PHASE 0: INITIALIZATION ===
    \node (beta) [init] {Init $\beta_{i,r,m}^+ = \beta_{i,r,m}^- = 1$\\(20 Beta distributions)};
    \node (alpha) [init, right=1.5cm of beta] {Init $\alpha_{i,r} = 1/R$\\$[0.25, 0.25, 0.25, 0.25]$};
    
    % === PHASE 1: CONTEXT ===
    \node (regime) [process, below=1.8cm of $(beta)!0.5!(alpha)$] {Sample regime $r_t \sim \pi(r)$\\Example: $r_t = \text{Bull}$};
    \node (filter) [process, below=0.8cm of regime] {Filter beliefs for $r_t$\\(5 Beta dists for Bull)};
    
    % === PHASE 2: SELECTION ===
    \node (thompson) [compute, below=1.2cm of filter] {Thompson Sampling:\\$\tilde{\theta}_m \sim \text{Beta}(\beta^+_{r_t,m}, \beta^-_{r_t,m})$};
    \node (select) [compute, below=0.8cm of thompson] {Select method:\\$m_i = \arg\max_m \tilde{\theta}_m$};
    
    % === PHASE 3: EXECUTION ===
    \node (execute) [process, below=1.2cm of select] {Execute method $m_i$\\in environment};
    \node (reward) [process, below=0.8cm of execute] {Raw reward:\\$R_i = A(r_t, m_i) + \epsilon$};
    
    % === PHASE 4: COMPETITION ===
    \node (bonus) [compute, below=1.2cm of reward] {Niche bonus:\\$B_i = \lambda(\alpha_{i,r_t} - 1/R)$};
    \node (score) [compute, below=0.8cm of bonus] {Final score:\\$S_i = R_i + B_i$};
    \node (compare) [decision, below=1.0cm of score] {$S_i > S_j$ $\forall j$?};
    
    % === PHASE 5: UPDATES ===
    \node (losernode) [loser, right=2.5cm of compare] {LOSER\\(No update)};
    \node (skillup) [winner, below left=1.5cm and 0cm of compare] {Skill update:\\$\beta^+_{r_t,m_i} \mathrel{+}= 1$};
    \node (affup) [winner, below right=1.5cm and 0cm of compare] {Affinity update:\\$\alpha_{r_t} \mathrel{+}= \eta(1-\alpha_{r_t})$};
    \node (normalize) [winner, below=0.8cm of $(skillup)!0.5!(affup)$] {Normalize: $\alpha \leftarrow \alpha / \|\alpha\|_1$};
    
    % === LOOP BACK ===
    \node (nextiter) [process, below=0.8cm of normalize] {Next iteration $t \leftarrow t+1$};
    
    % === ARROWS ===
    \draw [arrow] (beta) -- (regime);
    \draw [arrow] (alpha) -- (regime);
    \draw [arrow] (regime) -- (filter);
    \draw [arrow] (filter) -- (thompson);
    \draw [arrow] (thompson) -- (select);
    \draw [arrow] (select) -- (execute);
    \draw [arrow] (execute) -- (reward);
    \draw [arrow] (reward) -- (bonus);
    \draw [arrow] (bonus) -- (score);
    \draw [arrow] (score) -- (compare);
    \draw [arrow] (compare) -- node[above, font=\footnotesize] {No} (losernode);
    \draw [arrow] (compare) -| node[near start, left, font=\footnotesize] {Yes} (skillup);
    \draw [arrow] (compare) -| (affup);
    \draw [arrow] (skillup) -- (normalize);
    \draw [arrow] (affup) -- (normalize);
    \draw [arrow] (normalize) -- (nextiter);
    
    % Loop back arrow
    \draw [arrow, rounded corners] (nextiter.west) -- +(-3.5,0) |- (regime.west);
    
    % Phase labels
    \node [above=0.3cm of beta, font=\footnotesize\bfseries, blue!70] {PHASE 0: INIT};
    \node [left=0.1cm of regime, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 1};
    \node [left=0.1cm of thompson, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 2};
    \node [left=0.1cm of execute, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 3};
    \node [left=0.1cm of bonus, font=\footnotesize\bfseries, rotate=90, anchor=south] {PHASE 4};
    \node [left=0.1cm of skillup, font=\footnotesize\bfseries, rotate=90, anchor=south, green!50!black] {PHASE 5};
    
    \end{tikzpicture}
    }
    \caption{Complete NichePopulation algorithm flowchart. Blue nodes = initialization, Purple nodes = computation, Green nodes = winner-only updates, Red node = loser path (frozen state). The loop repeats for $T=500$ iterations.}
    \label{fig:workflow}
\end{figure}

% ============================================================================
% SECTION 2: INITIALIZATION DEEP DIVE
% ============================================================================

\subsubsection{Phase 0: Initialization (The Blank Slate)}

All $N=8$ agents begin \textbf{identically}---perfect generalists with no preferences.

\paragraph{Agent Memory Structure.} Each agent maintains two data structures:

\begin{enumerate}
    \item \textbf{Method Beliefs ($\beta$)}: A $R \times M$ matrix of Beta distributions (4 regimes $\times$ 5 methods = 20 distributions per agent)
    \item \textbf{Niche Affinity ($\alpha$)}: A probability vector over $R$ regimes
\end{enumerate}

\begin{table}[h]
\centering
\small
\caption{Initial state of all 8 agents at $t=0$. All agents are identical generalists.}
\begin{tabular}{l|cccc|c|l}
\toprule
\textbf{Agent} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} & \textbf{Status} \\
\midrule
Agent 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 1 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 2 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 3 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 4 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 5 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 6 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
Agent 7 & 0.25 & 0.25 & 0.25 & 0.25 & 0.000 & Generalist \\
\bottomrule
\end{tabular}
\label{tab:init}
\end{table}

\paragraph{Why Beta(1,1)?} The Beta(1,1) distribution is uniform over $[0,1]$, representing \textit{complete ignorance}---the agent assigns equal probability to all possible success rates. As the agent accumulates experience, the distribution narrows around the true success rate.

% ============================================================================
% SECTION 3: GROUND TRUTH AFFINITY MATRIX
% ============================================================================

\subsubsection{The Ground Truth: Regime-Method Affinity Matrix}

The simulation uses an \textbf{affinity matrix} $A(r, m) \in [0,1]$ that defines how well each method performs in each regime. This is the ``ground truth'' that agents must discover through experience.

\begin{table}[h]
\centering
\small
\caption{Crypto domain affinity matrix. Bold values indicate the best method for each regime.}
\begin{tabular}{l|cccc|l}
\toprule
\textbf{Method} & \textbf{Bull} & \textbf{Bear} & \textbf{Sideways} & \textbf{Volatile} & \textbf{Best For} \\
\midrule
Naive & 0.50 & 0.30 & 0.60 & 0.40 & --- \\
Momentum-Short & 0.80 & 0.70 & 0.40 & 0.60 & --- \\
Momentum-Long & \textbf{0.90} & \textbf{0.80} & 0.30 & 0.50 & Bull, Bear \\
Mean-Revert & 0.30 & 0.40 & \textbf{0.90} & \textbf{0.70} & Sideways, Volatile \\
Trend & 0.85 & 0.75 & 0.35 & 0.50 & --- \\
\bottomrule
\end{tabular}
\label{tab:affinity}
\end{table}

\textbf{Key insight}: Different methods are optimal in different regimes. This heterogeneity creates the \textit{opportunity} for specialization---agents who learn which methods work where will outperform generalists.

% ============================================================================
% SECTION 4: COMPLETE SINGLE ITERATION TRACE
% ============================================================================

\subsubsection{Complete Single Iteration Trace (All 8 Agents)}

We now trace through \textbf{one complete iteration} showing all 8 agents' computations.

\paragraph{Step 1: Environment samples regime.} $r_t = \text{Bull}$ (sampled with probability $\pi(\text{Bull}) = 0.30$)

\paragraph{Step 2: Each agent selects a method via Thompson Sampling.}

\begin{table}[h]
\centering
\small
\caption{Iteration 1: Method selection via Thompson Sampling (all agents start with Beta(1,1) for all methods).}
\begin{tabular}{l|ccccc|c}
\toprule
\textbf{Agent} & $\tilde{\theta}_{\text{naive}}$ & $\tilde{\theta}_{\text{mom-s}}$ & $\tilde{\theta}_{\text{mom-l}}$ & $\tilde{\theta}_{\text{mr}}$ & $\tilde{\theta}_{\text{trend}}$ & \textbf{Selected} \\
\midrule
Agent 0 & 0.42 & 0.68 & 0.55 & 0.31 & \textbf{0.73} & Trend \\
Agent 1 & 0.51 & 0.44 & \textbf{0.82} & 0.29 & 0.61 & Mom-Long \\
Agent 2 & 0.38 & \textbf{0.77} & 0.69 & 0.45 & 0.52 & Mom-Short \\
Agent 3 & 0.63 & 0.41 & 0.58 & \textbf{0.71} & 0.39 & Mean-Rev \\
Agent 4 & \textbf{0.85} & 0.33 & 0.47 & 0.52 & 0.44 & Naive \\
Agent 5 & 0.29 & 0.56 & 0.64 & 0.48 & \textbf{0.78} & Trend \\
Agent 6 & 0.44 & 0.62 & \textbf{0.71} & 0.35 & 0.59 & Mom-Long \\
Agent 7 & 0.57 & \textbf{0.79} & 0.66 & 0.42 & 0.51 & Mom-Short \\
\bottomrule
\end{tabular}
\end{table}

\textit{Note: With uniform priors, samples are random. Early diversity comes from sampling noise.}

\paragraph{Step 3: Execute methods and receive raw rewards.}

Raw reward formula: $R_i = A(r_t, m_i) + \epsilon_i$, where $\epsilon_i \sim \mathcal{N}(0, 0.15^2)$

\begin{table}[h]
\centering
\small
\caption{Iteration 1: Raw rewards and score calculation ($\lambda = 0.3$, all $\alpha_{r_t} = 0.25$, so all bonuses = 0).}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
\textbf{Agent} & \textbf{Method} & $A(r_t, m)$ & $\epsilon$ & $R_i$ & $B_i$ & $S_i = R_i + B_i$ \\
\midrule
Agent 0 & Trend & 0.85 & +0.03 & 0.88 & 0.00 & 0.88 \\
Agent 1 & Mom-Long & 0.90 & +0.07 & \textbf{0.97} & 0.00 & \textbf{0.97} $\leftarrow$ Winner \\
Agent 2 & Mom-Short & 0.80 & -0.05 & 0.75 & 0.00 & 0.75 \\
Agent 3 & Mean-Rev & 0.30 & +0.02 & 0.32 & 0.00 & 0.32 \\
Agent 4 & Naive & 0.50 & +0.11 & 0.61 & 0.00 & 0.61 \\
Agent 5 & Trend & 0.85 & -0.08 & 0.77 & 0.00 & 0.77 \\
Agent 6 & Mom-Long & 0.90 & -0.02 & 0.88 & 0.00 & 0.88 \\
Agent 7 & Mom-Short & 0.80 & +0.04 & 0.84 & 0.00 & 0.84 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Step 4: Winner-Take-All.} Agent 1 wins with $S_1 = 0.97$. \textbf{Only Agent 1 updates.}

\paragraph{Step 5: Winner's updates.}

\textbf{(a) Method belief update:}
\begin{equation}
\beta^+_{1,\text{Bull},\text{Mom-Long}} = 1 + 1 = 2, \quad \beta^-_{1,\text{Bull},\text{Mom-Long}} = 1 \text{ (unchanged)}
\end{equation}
Agent 1's belief for (Bull, Momentum-Long) shifts from Beta(1,1) to Beta(2,1), with mean $\frac{2}{3} = 0.67$.

\textbf{(b) Niche affinity update:}
\begin{align}
\alpha_{1,\text{Bull}}^{\text{new}} &= 0.25 + 0.1 \times (1 - 0.25) = 0.25 + 0.075 = 0.325 \\
\alpha_{1,r}^{\text{new}} &= \max(0.01, 0.25 - 0.1/3) = 0.217 \quad \text{for } r \neq \text{Bull}
\end{align}

\textbf{(c) Normalize:}
\begin{equation}
\text{Sum} = 0.325 + 3 \times 0.217 = 0.976, \quad \alpha_1 = (0.333, 0.222, 0.222, 0.222)
\end{equation}

\begin{table}[h]
\centering
\small
\caption{Agent states after Iteration 1. Only Agent 1 has changed.}
\begin{tabular}{l|cccc|c|l}
\toprule
\textbf{Agent} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} & \textbf{Change} \\
\midrule
Agent 0 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
\rowcolor{green!10} Agent 1 & \textbf{0.333} & 0.222 & 0.222 & 0.222 & 0.041 & $\uparrow$ Bull \\
Agent 2 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 3 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 4 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 5 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 6 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
Agent 7 & 0.250 & 0.250 & 0.250 & 0.250 & 0.000 & --- \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% SECTION 5: MULTI-ITERATION CONVERGENCE
% ============================================================================

\subsubsection{Multi-Iteration Convergence: How Agents Diverge}

The key phenomenon: \textbf{random early wins create path dependence}. Different agents win in different regimes, accumulating different experiences. Over 500 iterations, they diverge into specialists.

\begin{table}[h]
\centering
\small
\caption{Agent specialization trajectories across iterations. Each agent develops a distinct primary niche.}
\begin{tabular}{l|c|cccc|c}
\toprule
& \textbf{Iter} & $\alpha_{\text{Bull}}$ & $\alpha_{\text{Bear}}$ & $\alpha_{\text{Side}}$ & $\alpha_{\text{Vol}}$ & \textbf{SI} \\
\midrule
\multirow{4}{*}{\textbf{Agent 0}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.18 & 0.22 & \textbf{0.38} & 0.22 & 0.08 \\
& 200 & 0.12 & 0.15 & \textbf{0.58} & 0.15 & 0.31 \\
& 500 & 0.08 & 0.09 & \textbf{0.75} & 0.08 & 0.58 \\
\midrule
\multirow{4}{*}{\textbf{Agent 1}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & \textbf{0.42} & 0.19 & 0.20 & 0.19 & 0.12 \\
& 200 & \textbf{0.65} & 0.12 & 0.12 & 0.11 & 0.42 \\
& 500 & \textbf{0.82} & 0.06 & 0.06 & 0.06 & 0.71 \\
\midrule
\multirow{4}{*}{\textbf{Agent 2}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.20 & \textbf{0.35} & 0.23 & 0.22 & 0.06 \\
& 200 & 0.11 & \textbf{0.55} & 0.18 & 0.16 & 0.28 \\
& 500 & 0.05 & \textbf{0.78} & 0.10 & 0.07 & 0.62 \\
\midrule
\multirow{4}{*}{\textbf{Agent 3}} 
& 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0.00 \\
& 50 & 0.21 & 0.20 & 0.22 & \textbf{0.37} & 0.07 \\
& 200 & 0.14 & 0.13 & 0.15 & \textbf{0.58} & 0.30 \\
& 500 & 0.07 & 0.07 & 0.09 & \textbf{0.77} & 0.60 \\
\bottomrule
\end{tabular}
\label{tab:convergence}
\end{table}

\textbf{Observation}: After 500 iterations, Agent 1 is a Bull specialist (SI=0.71), Agent 2 is a Bear specialist (SI=0.62), Agent 0 is a Sideways specialist (SI=0.58), and Agent 3 is a Volatile specialist (SI=0.60). The population has \textit{partitioned} the regime space.

% ============================================================================
% SECTION 6: WHY LAMBDA=0 STILL WORKS
% ============================================================================

\subsubsection{Why $\lambda = 0$ Still Produces Specialization}

\textbf{Core question}: If there's no niche bonus ($\lambda = 0$), why do agents still specialize?

\textbf{Answer}: Competition alone creates path dependence.

\begin{enumerate}
    \item \textbf{Random initial wins}: In early iterations, winners are determined by noise. By chance, different agents win in different regimes.
    \item \textbf{Winner-take-all prevents convergence}: Losers don't update, so they can't copy the winner.
    \item \textbf{Method learning creates skill gaps}: Winners accumulate better beliefs about what works in ``their'' regime.
    \item \textbf{Skill gaps persist}: Once Agent A is slightly better at Bull, they win more Bull rounds, further improving their Bull skills.
\end{enumerate}

\begin{table}[h]
\centering
\small
\caption{SI comparison: $\lambda = 0$ vs $\lambda = 0.3$ across domains. Competition alone ($\lambda = 0$) still produces SI significantly above random baseline (0.13).}
\begin{tabular}{l|cc|c}
\toprule
\textbf{Domain} & $\lambda = 0$ SI & $\lambda = 0.3$ SI & Improvement \\
\midrule
Crypto & 0.31 & 0.75 & +142\% \\
Commodities & 0.30 & 0.78 & +160\% \\
Weather & 0.31 & 0.71 & +129\% \\
Solar & 0.26 & 0.82 & +215\% \\
Traffic & 0.29 & 0.68 & +134\% \\
Air Quality & 0.50 & 0.80 & +60\% \\
\midrule
\textbf{Average} & \textbf{0.33} & \textbf{0.76} & +140\% \\
\textbf{vs Random (0.13)} & \textbf{+154\%} & \textbf{+485\%} & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Conclusion}: The niche bonus \textit{accelerates} specialization but does not \textit{cause} it. Competition alone is sufficient.

% ============================================================================
% SECTION 7: COMPETITION DYNAMICS
% ============================================================================

\subsubsection{Competition Dynamics: Crowding and Migration}

\textbf{What happens if two agents specialize in the same regime?}

If Agents 1 and 6 both become Bull specialists, they compete directly in Bull rounds:
\begin{equation}
\E[\text{Payoff for Agent 1 in Bull}] = \frac{V_{\text{Bull}}}{k_{\text{Bull}}} \quad \text{where } k_{\text{Bull}} = 2 \text{ (two agents)}
\end{equation}

Meanwhile, Agent 3 (the only Volatile specialist) gets:
\begin{equation}
\E[\text{Payoff for Agent 3 in Volatile}] = \frac{V_{\text{Volatile}}}{k_{\text{Volatile}}} = V_{\text{Volatile}} \quad \text{(no competition)}
\end{equation}

\textbf{Result}: It's more profitable to be the sole specialist in an uncrowded niche. Over time, one of the Bull agents will ``migrate'' to Volatile (or another empty niche) after repeatedly losing to the other Bull specialist.

This is \textbf{competitive exclusion}: identical strategies cannot stably coexist.

% ============================================================================
% SECTION 8: METRICS CALCULATION
% ============================================================================

\subsubsection{Metrics Calculation: Step-by-Step Examples}

\paragraph{Specialization Index (SI).}

For Agent 1 with $\alpha = (0.82, 0.06, 0.06, 0.06)$:

\textbf{Step 1}: Calculate entropy
\begin{align}
H(\alpha) &= -\sum_{r} \alpha_r \log \alpha_r \\
&= -0.82 \log(0.82) - 3 \times 0.06 \log(0.06) \\
&= -0.82 \times (-0.198) - 0.18 \times (-2.813) \\
&= 0.162 + 0.506 = 0.668 \text{ nats}
\end{align}

\textbf{Step 2}: Normalize by maximum entropy
\begin{equation}
H_{\max} = \log(4) = 1.386 \text{ nats}
\end{equation}

\textbf{Step 3}: Compute SI
\begin{equation}
\SI = 1 - \frac{H(\alpha)}{H_{\max}} = 1 - \frac{0.668}{1.386} = 1 - 0.482 = \boxed{0.518}
\end{equation}

\textit{Interpretation: Agent 1 is 51.8\% specialized (on a scale where 0 = generalist, 1 = perfect specialist).}

\paragraph{Population Mean SI.}

With 8 agents having SI values $(0.71, 0.62, 0.58, 0.60, 0.55, 0.68, 0.64, 0.59)$:
\begin{equation}
\overline{\SI} = \frac{1}{8} \sum_{i=1}^{8} \SI_i = \frac{4.97}{8} = 0.621
\end{equation}

% ============================================================================
% SECTION 9: SUMMARY DIAGRAM
% ============================================================================

\subsubsection{Summary: The Complete Picture}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={rectangle, draw, rounded corners, minimum width=3.5cm, minimum height=0.8cm, align=center, font=\small},
    arrow/.style={-{Stealth[length=2.5mm]}, thick},
]

% Left column: Inputs
\node (env) [box, fill=blue!10] {Environment\\(4 regimes)};
\node (methods) [box, fill=blue!10, below=of env] {Methods\\(5 strategies)};
\node (agents) [box, fill=blue!10, below=of methods] {Agents\\(8 identical)};

% Middle: Mechanism
\node (compete) [box, fill=orange!15, right=2.5cm of methods] {Competition\\(winner-take-all)};
\node (update) [box, fill=orange!15, below=of compete] {Selective Update\\(only winner learns)};
\node (affinity) [box, fill=orange!15, above=of compete] {Niche Affinity\\(regime preference)};

% Right column: Outputs
\node (special) [box, fill=green!15, right=2.5cm of compete] {Specialization\\(SI $\approx$ 0.75)};
\node (diverse) [box, fill=green!15, above=of special] {Diversity\\(different niches)};
\node (robust) [box, fill=green!15, below=of special] {Robustness\\(+26.5\% perf)};

% Arrows
\draw [arrow] (env) -- (affinity);
\draw [arrow] (methods) -- (compete);
\draw [arrow] (agents) -- (update);
\draw [arrow] (affinity) -- (compete);
\draw [arrow] (compete) -- (update);
\draw [arrow] (update.east) -- ++(0.5,0) |- (affinity.east);
\draw [arrow] (compete) -- (special);
\draw [arrow] (special) -- (diverse);
\draw [arrow] (special) -- (robust);

% Labels
\node [above=0.2cm of env, font=\footnotesize\bfseries] {INPUTS};
\node [above=0.2cm of affinity, font=\footnotesize\bfseries] {MECHANISM};
\node [above=0.2cm of diverse, font=\footnotesize\bfseries] {OUTPUTS};

\end{tikzpicture}
\caption{High-level summary of NichePopulation: identical agents + competition $\rightarrow$ emergent specialization.}
\label{fig:summary}
\end{figure}

\paragraph{Key Takeaways.}
\begin{enumerate}
    \item \textbf{Competition is the source}: Specialization emerges from winner-take-all dynamics, not explicit diversity incentives.
    \item \textbf{No communication needed}: Agents differentiate through individual learning, not coordination.
    \item \textbf{Ecologically inspired}: The mechanism mirrors competitive exclusion in natural ecosystems.
    \item \textbf{Robust and general}: Works across 6 diverse real-world domains with consistent effect sizes.
\end{enumerate}

